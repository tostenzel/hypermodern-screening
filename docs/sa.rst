Sensitivity Analysis
====================

This section draws from [Saltelli.2004]_ and [Saltelli.2008]_.
They define sensitivity analysis as "the study of how uncertainty in the output of a model (numerical or otherwise) can be apportioned to different sources of uncertainty in the model input" ([Saltelli.2004]_, p. 42)). This apportioning implies a ranking of input parameters in terms of their importance for the model output. [Saltelli.2004] (2004, p. 52) define the most important parameter as "the one that [if fixed to its true, albeit unknown value]
would lead to the greatest reduction in the variance of the output Y." Therefore, a factor is not important if it influences the level of output :math:`Y` but rather its variance.

Sensitivity analysis includes different objectives. These have to be determined at first because the choice of methods depends on these objectives. Typically, the main and final goal is factor prioritisation. This is the aforementioned ranking of input parameters in terms of their importance. This ranking can be used to concentrate resources on data acquisition and estimation of a subset of parameters. The methods meeting the demands of factor prioritisation best are called quantitative. These typically require the highest computational effort.

There are multiple other objectives. The objective in this package is **screening** or factor fixing. It is basically the same as factor prioritisation except that it only aims to identify the input parameters that can be fixed at a given value without significantly reducing the output variance. Therefore, it focuses on the lowest parameters in the potential importance ranking. The reason why one would pursue factor fixing instead of factor prioritisation is computational costs. As factor fixing generates less information than factor prioritisation, less powerful methods can be applied. These methods require less computational resources and are called qualitative. Factor fixing can be used to prepare factor prioritisation for models that are more costly to evaluate. In this sense, it serves the same purpose as surrogate modelling.

Another important distinction is local versus global sensitivity analysis (GSA). It essentially refers to the applied methods. In fact, the definition by [Saltelli.2004]_  is already tailored to a global sensitivity analysis. In contrast to the given definition, "until quite recently, sensitivity analysis was [...] defined as a local measure of the effect of a given input on a given output" ([Saltelli.2004]_, p. 42)). This older definition differs from the definition used here in two aspects. First, it emphasises the level of output rather than its variance. Second, it describes the measure as a local one. The drawback of this approach becomes clear by considering an example of a local sensitivity measure. This measure is the so-called system derivate :math:`D_i = \frac{\partial Y}{\partial X_i}` ([Rabitz.1989]_). The derivative is typically computed at the mean, :math:`\overline{X_i}`, of the estimate for :math:`X_i`. :math:`D_i` is a so-called one-at-a-time measure because it changes only one factor. It has the following four drawbacks: first, it does not account for the interaction between multiple input parameters because it is one-at-a-time. Second, if the model derivation can not be derived analytically, the choice of the (marginal) change in :math:`X_i` is arbitrary. Third, the local derivative at :math:`\overline{X_i}` is only representative for the whole sample space of a random input if the model is linear in :math:`X_i`. Fourth, the measure does not relate to the output variance :math:`Var(Y)`. For these reasons, the field, its definitions and its methods have evolved beyond the notion of local sensitivity analysis. Yet, until recently, the main part of applications in different disciplines, such as physics ([Saltelli.2004]_ or economics ([Harenberg.2019]), still uses local measures.

The next to sections present quantitative and qualitative GSA. **Screening** measures belong to the latter.
